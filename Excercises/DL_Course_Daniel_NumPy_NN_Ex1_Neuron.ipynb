{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41aec9bb",
   "metadata": {},
   "source": [
    "<h2> Getting into Neural Networks! </h2> \n",
    "<h3> Ex. 1 </h3>\n",
    "<br>\n",
    "The goal of this excercise is to get you into two main pieces in NN:\n",
    "<ul>\n",
    "    <li>Activation Functions</li>\n",
    "    <li>Neurons (aka Perceptrons aka functions)</li>\n",
    "</ul>\n",
    "For this part, you will use solely NumPy!\n",
    "<br>\n",
    "Goal: \n",
    "<li> (1) Go over the activation main functions and understand their logic </li>\n",
    "<li> (2) Implement a neuron which gets an input, does a transformation and produces a single ouput on top of it \n",
    "    <br>\n",
    "    <b> Note: There are few inputs to \"pass\" in these functions </b>\n",
    "</li>\n",
    "<li> (3) Implement tanh using the formula which can easily found online, not using that built it function \n",
    "    <br>\n",
    "    <b> Note: There are few inputs to \"pass\" in these functions </b>\n",
    "</li>\n",
    "<li> (4) Implement a full neuron \"forward pass\" with a constant W0 and W1 (see below) </li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a142950",
   "metadata": {},
   "source": [
    "Let's read the functions of Sigmoid, TanH, ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bff2dff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Relu, Sigmoid, Tanh methods\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(X):\n",
    "    \"\"\"Implements sigmoid given x\"\"\"\n",
    "    return 1/(1+np.exp(-X))\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"Implements hyperbolic tan, given (x)\"\"\"\n",
    "    return np.tanh(x) # Note: Using this implementation, we can see that some functions are built in\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU function gets a value x and checks if it's lower than 0, hence returns 0, else returns the value x itself\"\"\"\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65414a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODOs:\n",
    "# 1. Implement tanh without the np.tanh build-in function, and check yourselves against it, using the below inputs:\n",
    "# 1a. x=[1, 6, 9]\n",
    "# 1b. x=[-1, 6, -10]\n",
    "# 1c. x=3\n",
    "\n",
    "# 2. Given the below inputs: \n",
    "# x1 = 1, \n",
    "# x2 = 2, \n",
    "# TODO: write a function that gets these inputs from and returns a number, which will be determined by all 3 options below:\n",
    "# Meaning: if sigmoid => value will be calculated by sigmoid\n",
    "         # if tanh => value will be calculated by tanh\n",
    "         # if relu => value will be calculated by relu\n",
    "# Assuming weights can be any static value, same for the bias, the formula should follow what we saw in the presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40b9f7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Implement the two below functions\n",
    "def full_neuron(X_0, X_1, W_0=0.1, W_1=0.2):\n",
    "    # TODO: Implement a multiplication as shown in class\n",
    "    pass\n",
    "\n",
    "def full_neuron_with_activation(X_0, X_1, W_0=-0.1, W_1=-0.2, activation='relu'):\n",
    "    # TODOs: \n",
    "    # (1) Implement multiplication as shown in class\n",
    "    # (2) Once implemented, pass the output from (1) in the proper activation function, as implemented above\n",
    "    # (3) Finally, return that final value\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
