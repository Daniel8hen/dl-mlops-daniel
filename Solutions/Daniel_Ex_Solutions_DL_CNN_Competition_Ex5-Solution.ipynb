{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69753143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shapes: (50000, 32, 32, 3)\n",
      "Test shapes: (10000, 32, 32, 3)\n",
      "Train shapes: (50000, 32, 32, 3)\n",
      "Test shapes: (10000, 32, 32, 3)\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-08 14:19:28.342269: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 614400000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6327 - accuracy: 0.4349 - val_loss: 1.4994 - val_accuracy: 0.4802\n",
      "Epoch 2/3\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.4098 - accuracy: 0.5174 - val_loss: 1.3605 - val_accuracy: 0.5344\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.2889 - accuracy: 0.5575 - val_loss: 1.3032 - val_accuracy: 0.5511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f47fc3a1ff0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras\n",
    "\n",
    "# Get data\n",
    "(train_images, train_labels),(test_images, test_labels) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "print(\"Train shapes:\", train_images.shape)\n",
    "print(\"Test shapes:\", test_images.shape)\n",
    "\n",
    "# Normalize the images.\n",
    "train_images = (train_images / 255) - 0.5\n",
    "test_images = (test_images / 255) - 0.5\n",
    "\n",
    "# Reshape the images.\n",
    "# train_images = np.expand_dims(train_images, axis=3)\n",
    "# test_images = np.expand_dims(test_images, axis=3)\n",
    "\n",
    "print(\"Train shapes:\", train_images.shape)\n",
    "print(\"Test shapes:\", test_images.shape)\n",
    "\n",
    "num_filters = 8\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "\n",
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(32, 32, 3)),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=3,\n",
    "  validation_data=(test_images, to_categorical(test_labels))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9a28650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 55ms/step\n",
      "[3 1 0 8 4]\n",
      "[[3]\n",
      " [8]\n",
      " [8]\n",
      " [0]\n",
      " [6]]\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk.\n",
    "model.save_weights('cnn.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('cnn.h5')\n",
    "\n",
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:5])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1))\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:5])\n",
    "\n",
    "# Doesn't seem the model is the best one\n",
    "# however we do have a baseline that works and that is the most important part here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3538c17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 1.3032 - accuracy: 0.5511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.303207278251648, 0.5511000156402588]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=test_images, y=to_categorical(test_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
