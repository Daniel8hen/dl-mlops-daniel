{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Dropout Excercise </h1>\n",
    "In the following exercise, you will have a chance to try out different dropout rates (p), and will be able to check which had the greatest effect on the model, in terms of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meet fashion MNIST - you will get familiar with this dataset soon...\n",
    "(train_images, train_labels),(test_images, test_labels) = keras.datasets.fashion_mnist.load_data()\n",
    "train_images = train_images /  255.0\n",
    "test_images = test_images / 255.0\n",
    "validation_images = train_images[:5000]\n",
    "validation_labels = train_labels[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another cool way to define networks is using Classes\n",
    "# this makes the process pretty much automatic, and not dependent on some static values\n",
    "### TODO: See the dropout rate below (0.2). \n",
    "# 1.Make sure to inject it as a parameter, so when we instantiate a model,\n",
    "# we will be able to define its droput rate accordingly.\n",
    "# 2. write a for loop of 3 models (no need for more),\n",
    "# 2.1 each one of the models should be running on the same optimizers and compile as below\n",
    "# 2.2 each one of the models should be running on the same optimizers and compile as below\n",
    "# 3. Once the for loop ended, make sure to plot graphs of training performance + test performance.\n",
    "# For that, you can use one of the old ex. we had.\n",
    "# Dropout values can be randomly picked (ranging between 0 - 1)\n",
    "### In other words, what I'm asking is a Hyperparameter random search, so you can implement this with some\n",
    "# python library you know.\n",
    "class CustomModel(keras.Model):\n",
    "    def __init__(self, dropout_rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_layer = keras.layers.Flatten(input_shape=(28,28))\n",
    "        self.hidden1 = keras.layers.Dense(200, activation='relu')\n",
    "        self.hidden2 = keras.layers.Dense(100, activation='relu')\n",
    "        self.hidden3 = keras.layers.Dense(60, activation='relu')\n",
    "        self.output_layer = keras.layers.Dense(10, activation='softmax')\n",
    "        self.dropout_layer = keras.layers.Dropout(rate=dropout_rate)\n",
    "    \n",
    "    def call(self, input, training=None):\n",
    "        input_layer = self.input_layer(input)\n",
    "        input_layer = self.dropout_layer(input_layer)\n",
    "        hidden1 = self.hidden1(input_layer)\n",
    "        hidden1 = self.dropout_layer(hidden1, training=training)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        hidden2 = self.dropout_layer(hidden2, training=training)\n",
    "        hidden3 = self.hidden3(hidden2)\n",
    "        hidden3 = self.dropout_layer(hidden3, training=training)\n",
    "        output_layer = self.output_layer(hidden3)\n",
    "        return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Note: epochs changed to 5 to make solution works faster, should work the same with 60\n",
    "for dropout_value in [0.1, 0.2, 0.3]: # For example\n",
    "    model = CustomModel(dropout_rate = dropout_value)\n",
    "    sgd = keras.optimizers.SGD(lr=0.01)\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=sgd, metrics=[\"accuracy\"])\n",
    "    \n",
    "    history = model.fit(train_images, train_labels, epochs=5, validation_data=(validation_images, validation_labels))\n",
    "\n",
    "    # plot\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate - as we discussed in class\n",
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
