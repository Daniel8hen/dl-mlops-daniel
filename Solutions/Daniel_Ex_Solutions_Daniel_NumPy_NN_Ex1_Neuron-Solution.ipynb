{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41aec9bb",
   "metadata": {},
   "source": [
    "<h2> Getting into Neural Networks! </h2> \n",
    "<h3> Ex. 1 </h3>\n",
    "<br>\n",
    "The goal of this excercise is to get you into two main pieces in NN:\n",
    "<ul>\n",
    "    <li>Activation Functions</li>\n",
    "    <li>Neurons (aka Perceptrons aka functions)</li>\n",
    "</ul>\n",
    "For this part, you will use solely NumPy!\n",
    "<br>\n",
    "Goal: \n",
    "<li> (1) Go over the activation main functions and understand their logic </li>\n",
    "<li> (2) Implement a neuron which gets an input, does a transformation and produces a single ouput on top of it \n",
    "    <br>\n",
    "    <b> Note: There are few inputs to \"pass\" in these functions </b>\n",
    "</li>\n",
    "<li> (3) Implement tanh using the formula which can easily found online, not using that built it function \n",
    "    <br>\n",
    "    <b> Note: There are few inputs to \"pass\" in these functions </b>\n",
    "</li>\n",
    "<li> (4) Implement a full neuron \"forward pass\" with a constant W0 and W1 (see below) </li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a142950",
   "metadata": {},
   "source": [
    "Let's read the functions of Sigmoid, TanH, ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bff2dff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Relu, Sigmoid, Tanh methods\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(X):\n",
    "    \"\"\"Implements sigmoid given x\"\"\"\n",
    "    return 1/(1+np.exp(-X))\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"Implements hyperbolic tan, given (x)\"\"\"\n",
    "    return np.tanh(x) # Btw, using this implementation, we can see that some functions are built in\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU function gets a value x and checks if it's lower than 0, hence returns 0, else returns the value x itself\"\"\"\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65414a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODOs:\n",
    "# 1. Implement tanh without the np.tanh build-in function, and check yourselves against it, using the below inputs:\n",
    "# 1a. x=[1, 6, 9]\n",
    "# 1b. x=[-1, 6, -10]\n",
    "# 1c. x=3\n",
    "\n",
    "# 2. Given the below inputs: \n",
    "# x1 = 1, \n",
    "# x2 = 2, \n",
    "# TODO: write a function that gets these inputs from and returns a number, which will be determined by all 3 options below:\n",
    "# Meaning: if sigmoid => value will be calculated by sigmoid\n",
    "         # if tanh => value will be calculated by tanh\n",
    "         # if relu => value will be calculated by relu\n",
    "# Assuming weights can be any static value, same for the bias, the formula should follow what we saw in the presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c02504fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.76159416,  0.99998771, -1.        ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sol 1\n",
    "def origin_tanh(x):\n",
    "    mone = np.exp(x) - np.exp(-x)\n",
    "    mehane = np.exp(x) + np.exp(-x)\n",
    "    return np.array(mone / mehane)\n",
    "\n",
    "x = [-1, 6, -10]\n",
    "# x = [1, 6, 9]\n",
    "# x = 3\n",
    "origin_tanh(np.array(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dce8a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z is: 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sol 2\n",
    "x1 = 1\n",
    "x2 = 2\n",
    "\n",
    "def neuron_and_activation(x1, x2, activation = 'sigmoid'):\n",
    "    # print(x1.shape)\n",
    "    w0 = 0\n",
    "    w1 = 6\n",
    "    w2 = 5\n",
    "    z = w0 + (w1 * x1) + (w2 * x2)\n",
    "    print(\"Z is:\", z)\n",
    "    if activation == 'sigmoid':\n",
    "        return sigmoid(z)\n",
    "    elif activation == 'relu':\n",
    "        return relu(z)\n",
    "    elif activation == 'tanh':\n",
    "        return tanh(z)\n",
    "    \n",
    "neuron_and_activation(x1, x2, 'relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feccef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sol 3\n",
    "def full_neuron(X_0, X_1, W_0=0.1, W_1=0.2):\n",
    "    # TODO: Implement a multiplication as shown in class\n",
    "    Z = X_0 * W_0 + X_1 * W_1\n",
    "    return Z\n",
    "\n",
    "\n",
    "# Sol 4\n",
    "def full_neuron_with_activation(X_0, X_1, W_0=-0.1, W_1=-0.2, activation='relu'):\n",
    "    # TODOs: \n",
    "    Z = full_neuron(X_0, X_1, W_0, W_1)\n",
    "    if activation == 'relu':\n",
    "        output = relu(Z)\n",
    "    return output\n",
    "    # (1) Implement multiplication as shown in class\n",
    "    # (2) Once implemented, pass the output from (1) in the proper activation function, as implemented above\n",
    "    # (3) Finally, return that final value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
